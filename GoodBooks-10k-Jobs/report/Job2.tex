\section{Job2: Correlazione tra rating e bookmarks}

L'obbiettivo di questo Job è quello di vedere se esiste una correlazione tra rating medio e bookmarks
Quindi il cuore del Job è il calcolo del rating e dei bookmarks e fare un confronto incrociato.

\subsection{Pianificazione}
Le tabelle che entrano in gioco per questo job sono le seguenti:
\begin{itemize}
    \item \texttt{ratings.csv}
    \item \texttt{to\_read.csv}
\end{itemize}

La prima verrà utilizzata per calcolare il rating medio mentre la seconda verrà utilizzata per calcolare i Bookmarks.
Dopo di che i risultati ottenuti saranno uniti in una unica tabella e da li verranno calcolati i risultati.

Ogni libro può quindi appartenere a uno dei 4 gruppi:
\begin{itemize}
    \item \textit{Molti Bookmarks e alto rating}
    \item \textit{Molti Bookmarks e basso rating}
    \item \textit{Pochi Bookmarks e basso rating}
    \item \textit{Pochi Bookmarks e alto rating}
\end{itemize}

\subsection{Hadoop MapReduce}

Per risolvere l'esercizio sono state calcolate empiricamente le due soglie da utilizzare per classificare i libri.
Una volta ottenute queste soglie è stato possibile pianificare il numero di job necessari e il loro funzionamento.
Il problema si può dividere in quattro fasi:

\begin{itemize}
    \item \textbf{Calcolare il rating} In questa fase bisogna leggere la tabella rating, eliminare gli attributi non necessari
    e calcolare la media delle valutazioni per ogni libro.
    \item \textbf{Calcolare i bookmarks} In questa fase bisogna leggere la tabella to_read, eliminare gli attributi non necessari
    e calcolare la somma del numero degli utenti che desiderano leggere il libro,tutto questo per ogni libro.
    \item \textbf{Classificare per rating} In questa fase si classificano i libri comparando il rating calcolato
    precedentemente con la soglia ricavata empiricamente.
    \item \textbf{Classificare per bookmarks}In questa fase si classificano i libri comparando il numero degli utenti che li desiderano leggere
    con la soglia ricavata empiricamente.
     \item \textbf{Conteggio delle classi}In questa fase si conta il numero di record delle varie classi al fine di determinarne le più
     frequenti.
\end{itemize}

In fase di implementazione è stato possibile raggruppare questi step al fine di ottenere un numero minore di job, di conseguenza raggiungere
livelli di performance migliori.
L'esercizio è stato svolto aggregando le prime quattro fasi nel job computeAVGRatingJob,
mentre l'ultima è stata implmentata in counterJob.

\begin{itemize}
    \item \textbf{computeAVGRatingJob} Questo job legge entrambe le tabelle e realizza le fasi uno e due, per poi fare i calcoli delle fasi tre e quattro.
    Il jo si conclude con la creazione dell'output, composta dalla semplice classe di appartenenza di ogni libro.
    \item \textbf{counterJob} Questo job si occupa della lettura del file generato nel job precedente e di contare i valori in esso contenuto al
    fine di creare come file di output un file contenente il totale di libri per ogni classe.
\end{itemize}

\subsubsection{Implementazione}
Per implementare il primo job si è reso necessario lo sviluppo di due mapper, uno per ogni tabella.
Entrambi i mapper leggono dal file gli attributi rilevanti escludendo quelli non necessari al completamento dell'esercizio.
Nello specifico il mapper relativo alla tabella rating.csv crea come output una lista di book_id - rating per ogni libro, mentre l'altro
produce una lista di book_id per ogni volta che quel libro è presente nella lista dei libri da leggere di un utente.
Il reducer che è incaricato della gestione dell'output di questi due mapper è CounterRatingReducer, il quale si occupa anche della classificazione dei
libri creando un output contenente la classe di appartenenza di ogni libro.

Il secondo job si occupa semplicemente di contare il numero di libri che vengono classificati nelle quattro possibili classi.
Il mapper utilizzato da questo job è CountMapper e come chiave dell'output utilizza la classe di appartenenza del libro.
Il reducer utilizzato è CountReducer e scrive sul file di output il numero di volte che un libro è presente in una classe.
Per questo secondo job è possibile sfruttare al massimo quattro reducer, la causa è il numero di classi che possono avere i libri.
Il mapper infatti può produrre al massimo quattro tipi diversi di chiave, di conseguenza qualsiasi funzione si utilizza per il partizionamento
creerà al massimo quattro risultati diversi, limitando di fatto il numero di reducer che si possono sfruttare.


\subsection{SparkSQL}
Per questo problema si \è utilizzato a pieno SparkSQL affidandoci alle ottimizzazioni interne del framework. Il problema
\è stato affrontato tramite i seguenti step:
\begin{itemize}
    \item \texttt{Creazione DataFrame}. Come primo passo abbiamo sfruttato una feature di \texttt{spark2} per leggere
    i dati dal file .csv inferendo direttamente lo schema della tabella. Una volta letto il file viene automaticamente creato
    un oggetto DataFrame pronto all'uso.
    \item \texttt{Rating medio}. Per calcolare il rating medio non abbiamo bisogno di fare Join con altre tabelle ma ci basta utilizzare
    ratings. E' stata quindi utilizzata la seguente query per calcolare il rating:
    \begin{verbatim}
    SELECT book_id, AVG(rating) as avgRating FROM ratings
    GROUP BY book_id
    \end{verbatim}
    \item \texttt{Bookmarks}. Anche per calcolare i bookmarks bancia lanciare una query su una sola tabella:
    \begin{verbatim}
        SELECT book_id, count(user_id) as marks FROM bookmarks
        GROUP BY book_id
    \end{verbatim}
    \item \texttt{Join tra i risultati}. Ora bisogna unire i due risultati in una unica tabella. Per il join essendo le
    tabelle abbastanza piccole è stato usato un \texttt{broadcast join} sulla tabella con i rating.
    \item \texttt{Risultati}. Ora abbiamo una tabella con tutte le informazioni che ci servono.
    Ci basterà lanciare qualche query che filtra i risultati per rating e bookmarks e vedere quanti elementi sono in
    ogni gruppo.
\end{itemize}
\subsection{Conclusioni}

Realizzare questo task in SparkSQL è stato estremamente semplice sià per la semplicità in se del task sia per l'interfacca
intuitiva di SparkSQL. Il risultato finale viene calcolato in tempo abbastanza buoni grazie alle ottimizzazioni interne del framework e
al setup generale. \href{ } %%todo set up spark sql

